{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a841926",
   "metadata": {},
   "source": [
    "DATA COLLECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97d19d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n",
      "C:\\Users\\Dell\\AppData\\Local\\Temp/ipykernel_4916/2434208010.py:16: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(cdf)\n"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "crypto_symbols = ['BTC', 'ETH', 'LTC', 'FIL', 'BNB', 'SOL', 'AVAX', 'BUSD', 'DOGE', 'CELO', 'DENT', 'ADA', 'SRM', 'BTG', 'ETC', 'EOS', 'XRP', 'XLA', 'BCH', 'XMR', 'BTS']\n",
    "cryptocurrencies = ['BITCOIN', 'ETHEREUM', 'Litecoin', 'Filecoin', 'Solana', 'Avalanche', 'Binance USD', 'Dogecoin', 'Celo', 'Dent', 'USDT', 'Cardano', 'Serum', 'Bitcoin Gold', 'Ethereum Classic', 'ApeCoin', 'EOS', 'XRP', 'Ripple Alpha', 'Baby Ripple', 'Bitcoin Cash', 'Monero', 'BitShares']\n",
    "\n",
    "for coins in crypto_symbols:\n",
    "    coin_index = crypto_symbols.index(coins)\n",
    "\n",
    "    yf_data = yf.Ticker(f\"{coins}-USD\").history(start='2021-03-01', end='2022-06-05', interval=\"1d\")\n",
    "    cdf = pd.DataFrame(yf_data)\n",
    "    cdf['name'] = cryptocurrencies[coin_index]\n",
    "    cdf['symbol'] = coins\n",
    "    df = df.append(cdf)\n",
    "\n",
    "#(df.reset_index())\n",
    "df.to_csv('my_crypto_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c7442",
   "metadata": {},
   "source": [
    "DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed9585c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Date          Open          High           Low         Close  \\\n",
      "0     2021-03-01  45159.503906  49784.015625  45115.093750  49631.242188   \n",
      "1     2021-03-02  49612.105469  50127.511719  47228.843750  48378.988281   \n",
      "2     2021-03-03  48415.816406  52535.136719  48274.320312  50538.242188   \n",
      "3     2021-03-04  50522.304688  51735.089844  47656.929688  48561.167969   \n",
      "4     2021-03-05  48527.031250  49396.429688  46542.515625  48927.304688   \n",
      "...          ...           ...           ...           ...           ...   \n",
      "8353  2022-03-29      0.027302      0.029184      0.027218      0.028057   \n",
      "8354  2022-03-30      0.028051      0.029364      0.027476      0.029012   \n",
      "8355  2022-03-31      0.029011      0.030300      0.027658      0.027752   \n",
      "8356  2022-04-01      0.027758      0.029637      0.026641      0.028823   \n",
      "8357  2022-04-02      0.028824      0.030570      0.028728      0.029075   \n",
      "\n",
      "           Volume  Dividends  Stock Splits          name symbol  \n",
      "0     53891300112          0             0       BITCOIN    BTC  \n",
      "1     47530897720          0             0       BITCOIN    BTC  \n",
      "2     53220811975          0             0       BITCOIN    BTC  \n",
      "3     52343816680          0             0       BITCOIN    BTC  \n",
      "4     48625928883          0             0       BITCOIN    BTC  \n",
      "...           ...        ...           ...           ...    ...  \n",
      "8353      7387122          0             0  Bitcoin Cash    BTS  \n",
      "8354      5545394          0             0  Bitcoin Cash    BTS  \n",
      "8355      9156179          0             0  Bitcoin Cash    BTS  \n",
      "8356      7308619          0             0  Bitcoin Cash    BTS  \n",
      "8357      8685749          0             0  Bitcoin Cash    BTS  \n",
      "\n",
      "[8358 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pandas import read_csv\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"This function returns the cryptocurrency coin\"\"\"\n",
    "    cryptocoin_data = pd.read_csv(r'C:\\Users\\Dell\\PycharmProjects\\pythonProject6\\my_crypto_data.csv')\n",
    "\n",
    "    return cryptocoin_data\n",
    "\n",
    "\n",
    "# load_data()\n",
    "\n",
    "\n",
    "def load_parse_date_data():\n",
    "    parse_date_data = cryptocoin_data = pd.read_csv(r'C:\\Users\\Dell\\PycharmProjects\\pythonProject6\\my_crypto_data.csv',\n",
    "                                                    index_col='Date', parse_dates=True)\n",
    "\n",
    "    return parse_date_data\n",
    "\n",
    "\n",
    "load_parse_date_data()\n",
    "\n",
    "\n",
    "def btc():\n",
    "    data = load_parse_date_data()\n",
    "    BTC = data[(data['symbol'] == 'BTC') & (data['Close'])]\n",
    "\n",
    "    return BTC\n",
    "\n",
    "\n",
    "# btc()\n",
    "\n",
    "def eth():\n",
    "    data_eth = load_parse_date_data()\n",
    "    ETH = data_eth[(data_eth['symbol'] == 'ETH') & (data_eth['Close'])]\n",
    "\n",
    "    return ETH\n",
    "\n",
    "\n",
    "# eth()\n",
    "\n",
    "def ltc():\n",
    "    data_ltc = load_parse_date_data()\n",
    "    LTC = data_ltc[(data_ltc['symbol'] == 'LTC') & (data_ltc['Close'])]\n",
    "\n",
    "    return LTC['Close']\n",
    "\n",
    "\n",
    "# ltc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d16e2b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (0.11.2)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from seaborn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from seaborn) (1.8.0)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from seaborn) (1.22.2)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from seaborn) (3.5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from matplotlib>=2.2->seaborn) (4.33.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from matplotlib>=2.2->seaborn) (9.1.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.4.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from matplotlib>=2.2->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from matplotlib>=2.2->seaborn) (3.0.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from pandas>=0.23->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dell\\downloads\\anaconda\\envs\\tf-gpu\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de384856",
   "metadata": {},
   "source": [
    "DATA UNDERSTANDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a52accbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "from scipy.stats import shapiro\n",
    "from numpy import percentile\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def data_info():\n",
    "    \"\"\"The data info function returns the type of each feature in the dataset\"\"\"\n",
    "    data_types = load_data().dtypes\n",
    "    return data_types\n",
    "\n",
    "\n",
    "# data_info()\n",
    "\n",
    "\n",
    "def numerical_variables():\n",
    "    \"\"\"This function returns the numerical variables in a bid to segregate them from the categorical variable\"\"\"\n",
    "    numerical = load_data().select_dtypes(\"number\")\n",
    "    return numerical\n",
    "\n",
    "\n",
    "# numerical_variables()\n",
    "\n",
    "\n",
    "def categorical_variables():\n",
    "    \"\"\"This function returns the categorical variables in the dataset\"\"\"\n",
    "    categorical = load_data().select_dtypes(\"object\")\n",
    "    return categorical\n",
    "\n",
    "\n",
    "# categorical_variables()\n",
    "\n",
    "\n",
    "def numerical_description():\n",
    "    numerical_desc = numerical_variables().describe().round(0)\n",
    "    return numerical_desc\n",
    "\n",
    "\n",
    "# numerical_description()\n",
    "\n",
    "\n",
    "def numerical_missing_values_percent():\n",
    "    \"\"\"The percentage of missing values in numerical variables would be returned by this function.\"\"\"\n",
    "    missing_feature_percent = numerical_variables().isnull().sum() / (len(process.load_data())) * 100\n",
    "    return missing_feature_percent\n",
    "\n",
    "\n",
    "# numerical_missing_values_percent()\n",
    "\n",
    "\n",
    "def continuous_unique_variables():\n",
    "    \"\"\"The cardinality check for the numerical variables  is what this function returns. The Dividends and Stock Split variables have one cardinality\"\"\"\n",
    "    cardinality = numerical_variables().apply(pd.Series.nunique)\n",
    "    return cardinality\n",
    "\n",
    "\n",
    "# continuous_unique_variables()\n",
    "\n",
    "\n",
    "def categorical_variables_missing_data():\n",
    "    \"\"\"The 'categorical variables missing data' function returns the percentage of missing values in the categorical features\"\"\"\n",
    "    categorical_missing_check = categorical_variables().isnull().sum() / (len(process.load_data())) * 100\n",
    "    return categorical_missing_check\n",
    "\n",
    "\n",
    "# categorical_variables_missing_data()\n",
    "\n",
    "\n",
    "def categorical_variables_cardinality_check():\n",
    "    \"\"\"This function returns the cardinality of the categorical variables. There are 398 unique date and 23 distinct cryptocurrencies.\"\"\"\n",
    "    categorical_cardinality = categorical_variables().apply(pd.Series.nunique)\n",
    "    return categorical_cardinality\n",
    "\n",
    "\n",
    "# categorical_variables_cardinality_check()\n",
    "\n",
    "\n",
    "def skewness():\n",
    "    \"\"\"This function returns the skewness of each numerical values\"\"\"\n",
    "    numerical_skewness = numerical_variables().skew()\n",
    "    return numerical_skewness\n",
    "\n",
    "\n",
    "# skewness()\n",
    "\n",
    "def open_normality_check():\n",
    "    \"\"\"This distribution is not normal\"\"\"\n",
    "    data = numerical_variables()\n",
    "    pyplot.hist(data['Open'])\n",
    "    pyplot.xlabel('Open Price')\n",
    "    pyplot.ylabel('Frequency')\n",
    "    pyplot.title('Open Price Distribution')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "# open_normality_check()\n",
    "\n",
    "def close_normality_check():\n",
    "    \"\"\"The distribution is normal\"\"\"\n",
    "    data = numerical_variables()\n",
    "    pyplot.hist(data['Close'])\n",
    "    pyplot.xlabel('Cse Price')\n",
    "    pyplot.ylabel('Frequency')\n",
    "    pyplot.title('Close Price Distribution')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "# close_normality_check()\n",
    "\n",
    "def high_normality_check():\n",
    "    data = numerical_variables()\n",
    "    pyplot.hist(data['High'])\n",
    "    pyplot.xlabel('High Price')\n",
    "    pyplot.ylabel('Frequency')\n",
    "    pyplot.title('High Price Distribution')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "#high_normality_check()\n",
    "\n",
    "def low_normality_check():\n",
    "    data = numerical_variables()\n",
    "    pyplot.hist(data['Low'])\n",
    "    pyplot.xlabel('Low Price')\n",
    "    pyplot.ylabel('Frequency')\n",
    "    pyplot.title('Low Price Distribution')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "# low_normality_check()\n",
    "\n",
    "\n",
    "def volume_normality_check():\n",
    "    data = numerical_variables()\n",
    "    pyplot.hist(data['Volume'])\n",
    "    pyplot.xlabel('Volume')\n",
    "    pyplot.ylabel('Frequency')\n",
    "    pyplot.title('Volume Distribution')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "# volume_normality_check()\n",
    "\n",
    "\n",
    "def dividends():\n",
    "    data = numerical_variables()\n",
    "    pyplot.hist(data['Stock Splits'])\n",
    "    pyplot.xlabel('Dividends')\n",
    "    pyplot.ylabel('Frequency')\n",
    "    pyplot.title('Dividend Distribution')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "# dividends()\n",
    "\n",
    "def stock_splits():\n",
    "    data = numerical_variables()\n",
    "    pyplot.hist(data['Stock Splits'])\n",
    "    pyplot.xlabel('Stock Splits')\n",
    "    pyplot.ylabel('Frequency')\n",
    "    pyplot.title('Stock Splits Distribution')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "#stock_splits()\n",
    "\n",
    "\n",
    "# Statistical Test for Normality Tests: The Shapiro-Wilk Test for Normality would be used on the numerical variables.\n",
    "\n",
    "def shapiro_wilk_test():\n",
    "    \"\"\"This function  generate the statistical test for normality.\"\"\"\n",
    "    stat, p = shapiro(numerical_variables()['Low'])\n",
    "    print('The statistics=%.3f, p=%.3f' % (stat, p))\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('The sample is normally distributed (do not reject H0, the Null hypothesis)')\n",
    "    else:\n",
    "        print('The sample is not normally distributed (reject H0, the Null hypothesis)')\n",
    "\n",
    "\n",
    "#shapiro_wilk_test()\n",
    "\n",
    "\n",
    "def check_outliers():\n",
    "    \"\"\"Checking for outliers in the numerical features using the Inter-quartile Range Method (IQRM) since statistics shows the numerical features to be non-Gaussian \"\"\"\n",
    "    numeric_features = numerical_variables()[['Open', 'High', 'Low', 'Close', 'Volume', 'Dividends', 'Stock Splits']]\n",
    "    graphical_outlier = sns.boxplot(data=numeric_features, orient=\"h\", palette=\"Set2\")\n",
    "    graphical_outlier.set_ylabel('Features')\n",
    "    graphical_outlier.set_xlabel('Data Values')\n",
    "    graphical_outlier.set_ylabel('Features')\n",
    "    # return graphical_outlier\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "#check_outliers()\n",
    "\n",
    "\n",
    "def multi_variate_analysis():\n",
    "    \"\"\"This is to explore the relationship between two or more variables for the machine learning task\"\"\"\n",
    "\n",
    "\n",
    "def correlation_heatmap():\n",
    "    corrmatrix = numerical_variables().corr()\n",
    "    coin_relation = corrmatrix.index\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    g = sns.heatmap(numerical_variables()[coin_relation].corr(), annot=True, cmap=\"Accent\")\n",
    "    plt.title('Correlation Plots Among Variables')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#correlation_heatmap()\n",
    "\n",
    "\n",
    "def time_series_analysis():\n",
    "    \"\"\"This analysis would be used to determine the stationarity of the dataset. To check how Price changes over time.\"\"\"\n",
    "    plt.style.use('default')\n",
    "    plt.figure(figsize=(16, 8), dpi=150)\n",
    "    #btc = btc()\n",
    "    #eth = eth()\n",
    "    #ltc = ltc()\n",
    "    btc()['Close'].plot(label='BTC', color='orange')\n",
    "    eth()['Close'].plot(label='ETH')\n",
    "    ltc()['Close'].plot(label='LTC')\n",
    "\n",
    "    plt.title('Close Price Plot')\n",
    "    plt.xlabel('Months')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#time_series_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10370a16",
   "metadata": {},
   "source": [
    "PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47915ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 35\n",
      "[array([37917.6015625 , 38483.125     , 38743.2734375 , 36952.984375  ,\n",
      "       37154.6015625 , 41500.875     , 41441.1640625 , 42412.43359375,\n",
      "       43840.28515625, 44118.4453125 , 44338.796875  , 43565.11328125,\n",
      "       42407.9375    , 42244.46875   , 42197.515625  , 42586.91796875,\n",
      "       44575.203125  , 43961.859375  , 40538.01171875, 40030.9765625 ,\n",
      "       40122.15625   , 38431.37890625, 37075.28125   , 38286.02734375,\n",
      "       37296.5703125 , 38332.609375  , 39214.21875   , 39105.1484375 ,\n",
      "       37709.78515625, 43193.234375  , 44354.63671875, 43924.1171875 ,\n",
      "       42451.7890625 , 39137.60546875, 39400.5859375 ]), array([2603.46655273, 2688.27880859, 2792.1171875 , 2682.85400391,\n",
      "       2679.16259766, 2983.58691406, 3014.64819336, 3057.47607422,\n",
      "       3142.47070312, 3122.60864258, 3239.45703125, 3077.48217773,\n",
      "       2927.38354492, 2917.36279297, 2883.46337891, 2933.47900391,\n",
      "       3179.87719727, 3127.83007812, 2881.48193359, 2785.72753906,\n",
      "       2763.70117188, 2628.6484375 , 2573.81616211, 2639.29931641,\n",
      "       2590.35961914, 2598.06713867, 2764.53564453, 2781.11181641,\n",
      "       2621.80175781, 2919.20117188, 2972.48510742, 2950.1184082 ,\n",
      "       2834.46899414, 2617.15600586, 2664.83105469]), array([108.79985809, 109.58769989, 115.40200043, 108.55602264,\n",
      "       110.28700256, 120.91803741, 121.91674042, 127.44550323,\n",
      "       137.10739136, 133.99124146, 140.17160034, 135.08839417,\n",
      "       126.03115845, 126.68367767, 126.204216  , 124.80005646,\n",
      "       131.76261902, 128.1608429 , 116.30241394, 115.19709015,\n",
      "       115.70900726, 110.83203888, 103.45049286, 107.52279663,\n",
      "       105.96595001, 104.23049164, 109.26631165, 108.24365997,\n",
      "       103.10559845, 113.47457886, 112.54404449, 110.35135651,\n",
      "       111.35211945, 101.39179993, 104.97718811]), array([19.83703041, 21.32290459, 20.91481209, 20.18340111, 20.31682205,\n",
      "       21.61750221, 22.01656914, 22.44775009, 24.40475464, 24.59631348,\n",
      "       24.26986122, 23.17950249, 21.03780174, 20.85038185, 20.66428566,\n",
      "       20.72755051, 22.34214783, 23.08382607, 20.48183632, 21.16025925,\n",
      "       20.87793732, 19.8388176 , 18.60088539, 19.21353149, 18.48812294,\n",
      "       18.42663574, 18.92191887, 19.07542038, 20.34723663, 21.67240715,\n",
      "       20.9356308 , 20.68394661, 20.14182663, 18.77859497, 19.10863876]), array([377.61291504, 375.27731323, 385.46380615, 367.47167969,\n",
      "       370.87774658, 399.02960205, 414.20001221, 419.54556274,\n",
      "       433.22927856, 408.77676392, 422.46554565, 415.11923218,\n",
      "       400.4223938 , 403.38973999, 398.88677979, 403.66079712,\n",
      "       432.40939331, 427.91812134, 402.45004272, 399.56918335,\n",
      "       400.0435791 , 380.82592773, 356.66125488, 374.3074646 ,\n",
      "       366.53948975, 361.23464966, 374.91494751, 373.63876343,\n",
      "       360.53103638, 395.61242676, 408.4822998 , 409.00030518,\n",
      "       402.5506897 , 374.28088379, 384.87335205]), array([ 93.29767609,  99.7388382 , 109.97898102, 101.83760071,\n",
      "       101.45903778, 112.17424774, 113.89430237, 115.29975891,\n",
      "       117.35514832, 113.59548187, 114.01937866, 106.20280457,\n",
      "        96.38188171,  95.91820526,  93.24410248,  96.43327332,\n",
      "       104.87330627, 101.93289948,  93.41042328,  90.06546021,\n",
      "        91.4966507 ,  90.90141296,  83.12316132,  86.46538544,\n",
      "        84.90278625,  89.19474792,  92.59882355,  90.14156342,\n",
      "        85.5190506 ,  99.5203476 ,  98.65355682, 100.64963531,\n",
      "        95.53919983,  88.24597931,  89.64784241]), array([67.9737854 , 69.99242401, 73.44881439, 67.87997437, 68.10771942,\n",
      "       77.31163788, 77.24948883, 78.75726318, 83.16699982, 86.62419891,\n",
      "       90.02086639, 91.13622284, 81.73299408, 81.24567413, 78.24311829,\n",
      "       82.01966858, 92.67250061, 95.34611511, 88.12377167, 83.8369751 ,\n",
      "       85.09931946, 77.33678436, 69.98892975, 74.57435608, 73.69863129,\n",
      "       76.52396393, 80.83261871, 81.87970734, 74.99118805, 84.3392334 ,\n",
      "       86.7359314 , 83.29166412, 78.55764771, 75.65424347, 76.72742462]), array([1.00098205, 1.00010002, 1.00088096, 0.998694  , 1.00127697,\n",
      "       0.99969298, 0.99883997, 0.99989802, 0.99927598, 1.00058305,\n",
      "       1.00034201, 0.99893802, 0.99895102, 0.99962002, 0.99866599,\n",
      "       0.998698  , 1.00044799, 0.999201  , 1.00060797, 0.99906099,\n",
      "       0.99979001, 0.99885303, 0.99849403, 1.00072801, 0.99973702,\n",
      "       0.99977797, 1.00057995, 1.000422  , 0.99922299, 1.00068998,\n",
      "       1.00033104, 0.999089  , 1.00072503, 1.00031495, 0.99977601]), array([0.13947099, 0.14180499, 0.14263099, 0.137235  , 0.137541  ,\n",
      "       0.147503  , 0.14714099, 0.15377   , 0.16557799, 0.15840501,\n",
      "       0.159123  , 0.151889  , 0.14484701, 0.14440501, 0.148948  ,\n",
      "       0.14600299, 0.151761  , 0.149095  , 0.138552  , 0.138768  ,\n",
      "       0.141206  , 0.136868  , 0.12849   , 0.13155299, 0.127846  ,\n",
      "       0.123813  , 0.12757599, 0.127647  , 0.123111  , 0.133156  ,\n",
      "       0.133938  , 0.13299   , 0.12961   , 0.122591  , 0.124996  ]), array([3.03029203, 3.16999292, 3.23397708, 3.02824211, 3.02578211,\n",
      "       3.2406311 , 3.24063492, 3.36664796, 3.52817702, 3.31245589,\n",
      "       3.44672203, 3.13077307, 3.05570197, 3.05371094, 2.94332409,\n",
      "       2.90310097, 3.17121506, 3.01136804, 2.83376002, 2.74520397,\n",
      "       2.72262192, 2.56742811, 2.4122591 , 2.52946091, 2.38801599,\n",
      "       2.2785759 , 2.39982104, 2.41350508, 2.36145711, 2.64371204,\n",
      "       2.67795491, 2.58418989, 2.55623507, 2.65787792, 2.854357  ]), array([0.002512, 0.002627, 0.002626, 0.002505, 0.002501, 0.002801,\n",
      "       0.002942, 0.003059, 0.003268, 0.003307, 0.003328, 0.003129,\n",
      "       0.002899, 0.002908, 0.002804, 0.002836, 0.00311 , 0.003035,\n",
      "       0.002758, 0.002663, 0.002685, 0.002552, 0.002334, 0.002402,\n",
      "       0.002335, 0.002279, 0.002404, 0.00238 , 0.002293, 0.002639,\n",
      "       0.002624, 0.002527, 0.002512, 0.002344, 0.00245 ]), array([1.03747296, 1.05230296, 1.088624  , 1.02890599, 1.05644405,\n",
      "       1.13562405, 1.12734497, 1.14454699, 1.19730699, 1.17872   ,\n",
      "       1.19481003, 1.15247798, 1.08034694, 1.05819905, 1.04581904,\n",
      "       1.05026698, 1.10758603, 1.08498096, 1.01989496, 0.99711299,\n",
      "       0.99770302, 0.93290198, 0.85823601, 0.88901699, 0.869169  ,\n",
      "       0.85342503, 0.898027  , 0.887918  , 0.85833502, 0.95982802,\n",
      "       0.96234602, 0.93866199, 0.902659  , 0.84288901, 0.86567199]), array([2.30935907, 2.31433105, 2.36847901, 2.25828695, 2.25151801,\n",
      "       2.42736602, 2.531353  , 2.55887294, 2.68890595, 2.55821705,\n",
      "       2.64385605, 2.48662305, 2.33125091, 2.27966499, 2.25969195,\n",
      "       2.26875305, 2.45159197, 2.43607807, 2.207582  , 2.16115189,\n",
      "       2.15277004, 2.06267095, 1.91123497, 1.95113504, 1.88136399,\n",
      "       1.83923697, 1.94465601, 1.97995603, 1.90357006, 2.08884597,\n",
      "       2.09130192, 2.07455492, 1.97607601, 1.83452404, 1.87580204]), array([29.61634827, 29.62545395, 29.91239929, 29.50370026, 29.65930557,\n",
      "       32.50956726, 33.07117462, 34.09070587, 36.14395523, 35.20979691,\n",
      "       36.17395782, 35.9116478 , 33.62769318, 33.00115585, 32.96675491,\n",
      "       32.54729462, 34.26016617, 33.87879944, 31.50341415, 30.80136108,\n",
      "       30.23965073, 29.16626549, 27.48212624, 28.1492939 , 28.04890442,\n",
      "       27.73696327, 28.97377777, 30.58506966, 29.25064659, 31.90309525,\n",
      "       32.69284058, 31.55937576, 31.11388779, 29.91573524, 29.87197113]), array([25.01261711, 25.71295547, 26.82476234, 26.31565857, 27.90536308,\n",
      "       29.69120407, 29.43660736, 29.8921299 , 32.6400528 , 32.09848785,\n",
      "       33.83422089, 34.45188522, 32.69767761, 31.6366272 , 32.6863327 ,\n",
      "       31.39624977, 33.47097778, 32.30659103, 29.39853287, 29.04769707,\n",
      "       28.76227951, 27.41193581, 25.03525352, 26.69472504, 26.5665226 ,\n",
      "       26.58348083, 28.02692032, 28.88312912, 27.37037086, 30.1151371 ,\n",
      "       30.31150055, 29.53998756, 30.21997452, 27.5830307 , 27.83560181]), array([2.31907892, 2.34201694, 2.35694909, 2.2803359 , 2.29807711,\n",
      "       2.48021293, 2.48028708, 2.54000306, 2.68913794, 2.63366199,\n",
      "       2.6693399 , 2.56111908, 2.41559792, 2.39493299, 2.38979506,\n",
      "       2.37290692, 2.52687812, 2.55372691, 2.32525992, 2.29921699,\n",
      "       2.28920293, 2.20611   , 2.08092403, 2.16894007, 2.11076498,\n",
      "       2.06846809, 2.14414406, 2.16137695, 2.0928781 , 2.2793951 ,\n",
      "       2.26443291, 2.21259594, 2.15981889, 2.00251102, 2.04351711]), array([0.603181  , 0.61914903, 0.629233  , 0.60148299, 0.60768402,\n",
      "       0.65587699, 0.66712701, 0.68364698, 0.82781899, 0.87819397,\n",
      "       0.87062699, 0.83001   , 0.76022601, 0.82240802, 0.81027299,\n",
      "       0.80221802, 0.85233301, 0.83903599, 0.76717299, 0.78588998,\n",
      "       0.822528  , 0.77829599, 0.703466  , 0.72283298, 0.69874603,\n",
      "       0.69627202, 0.76890302, 0.75097299, 0.72199202, 0.78204399,\n",
      "       0.78486198, 0.76849598, 0.75218499, 0.712672  , 0.75489998]), array([9.50000031e-05, 9.99999975e-05, 1.01999998e-04, 9.40000027e-05,\n",
      "       9.50000031e-05, 1.01999998e-04, 9.79999968e-05, 9.60000034e-05,\n",
      "       1.04999999e-04, 9.89999971e-05, 1.09000001e-04, 9.79999968e-05,\n",
      "       8.70000003e-05, 9.00000014e-05, 9.20000020e-05, 8.29999990e-05,\n",
      "       8.49999997e-05, 1.08000000e-04, 9.40000027e-05, 9.30000024e-05,\n",
      "       9.50000031e-05, 7.99999980e-05, 7.59999966e-05, 7.40000032e-05,\n",
      "       7.30000029e-05, 7.20000025e-05, 7.59999966e-05, 7.69999970e-05,\n",
      "       7.20000025e-05, 8.29999990e-05, 7.79999973e-05, 7.89999976e-05,\n",
      "       7.89999976e-05, 7.20000025e-05, 7.69999970e-05]), array([290.50073242, 285.53927612, 288.1484375 , 278.59036255,\n",
      "       279.97592163, 305.98312378, 320.87854004, 325.0093689 ,\n",
      "       342.79690552, 336.25402832, 345.83364868, 341.83203125,\n",
      "       320.55270386, 329.23687744, 334.87149048, 331.33441162,\n",
      "       343.13092041, 335.36160278, 312.81619263, 310.97793579,\n",
      "       312.02392578, 303.13562012, 284.34567261, 293.675354  ,\n",
      "       289.30834961, 300.81051636, 308.45904541, 314.53085327,\n",
      "       304.10360718, 337.12200928, 328.23138428, 320.40515137,\n",
      "       315.12631226, 289.91744995, 293.9798584 ]), array([147.91908264, 147.37472534, 146.16348267, 144.80357361,\n",
      "       146.63528442, 165.12031555, 168.68193054, 173.52580261,\n",
      "       184.63526917, 178.89982605, 185.62585449, 179.44432068,\n",
      "       168.67033386, 173.77044678, 174.39193726, 176.29377747,\n",
      "       182.37001038, 179.30949402, 163.38528442, 160.13787842,\n",
      "       161.28518677, 153.93527222, 147.96923828, 150.90545654,\n",
      "       149.12062073, 147.03302002, 153.46913147, 157.66094971,\n",
      "       150.72886658, 170.19030762, 177.53414917, 177.70994568,\n",
      "       172.03263855, 159.71348572, 165.77366638]), array([0.023964, 0.023977, 0.024595, 0.023239, 0.022525, 0.025045,\n",
      "       0.026203, 0.027182, 0.02802 , 0.027554, 0.027915, 0.026407,\n",
      "       0.024564, 0.024278, 0.023518, 0.023734, 0.025834, 0.025314,\n",
      "       0.022957, 0.022646, 0.022386, 0.021422, 0.019739, 0.020452,\n",
      "       0.019704, 0.019932, 0.02128 , 0.020597, 0.01957 , 0.021978,\n",
      "       0.023013, 0.02256 , 0.021555, 0.020155, 0.020273])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot, pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def preprocessing():\n",
    "    \"\"\"This module details the preprocessing of the data. Feature scaling, dimensionality reduction and encoding etc.\"\"\"\n",
    "\n",
    "\n",
    "def reduced_data():\n",
    "    \"\"\"This function is written to remove the columns with zeros: Dividends and Stock Splits. This is because they do not\n",
    "    present any meaningful information.\"\"\"\n",
    "    data = load_data()\n",
    "    cols = ['Dividends', 'Stock Splits']\n",
    "    mod_data = data.drop(cols, axis=1)\n",
    "    return mod_data\n",
    "\n",
    "\n",
    "# reduced_data()\n",
    "\n",
    "\n",
    "def numeric_values_of_reduced_data():\n",
    "    \"\"\"This function returns the numeric values of the function defined above\"\"\"\n",
    "    numeric_reduced_data = reduced_data()\n",
    "    altcoins_numeric_data = numeric_reduced_data.select_dtypes(\"number\")\n",
    "    return altcoins_numeric_data\n",
    "\n",
    "\n",
    "# numeric_values_of_reduced_data()\n",
    "\n",
    "\n",
    "def checking_missing_values():\n",
    "    \"\"\"This is to check if null values are present in the reduced data\"\"\"\n",
    "    missing_feature_percent = reduced_data().isnull().sum() / (len(process.load_data())) * 100\n",
    "\n",
    "    return missing_feature_percent\n",
    "\n",
    "\n",
    "# checking_missing_values()\n",
    "\n",
    "\n",
    "def feature_importance():\n",
    "    \"\"\"This function would rank the features according to their relevance in predicting the target variable\"\"\"\n",
    "    dataset = numeric_values_of_reduced_data()\n",
    "\n",
    "    # split dataset into X and y\n",
    "    X = dataset[['Open', 'High', 'Low', 'Volume']]\n",
    "    y = dataset['Close']\n",
    "\n",
    "    # instantiate the model\n",
    "    model = RandomForestRegressor()\n",
    "\n",
    "    # fit the model\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # plot the feature importance\n",
    "    importance = model.feature_importances_\n",
    "\n",
    "    for i, v in enumerate(importance):\n",
    "        print('Feature: %0d, Score: %.2f' % (i, v))\n",
    "\n",
    "    pyplot.bar([x for x in range(len(importance))], importance)\n",
    "    plt.title('Feature Importance Graph')\n",
    "    plt.xlabel('Index of the features')\n",
    "    plt.ylabel('(Feature Importance)-F1-Scores')\n",
    "    pyplot.show()\n",
    "\n",
    "\n",
    "#feature_importance()\n",
    "\n",
    "\n",
    "def selected_features():\n",
    "    \"\"\"This function returns the dimensionally reduced data\"\"\"\n",
    "    data = load_data()\n",
    "    cols = ['Dividends', 'Stock Splits', 'Volume']\n",
    "    mod_data = data.drop(cols, axis=1)\n",
    "    return mod_data\n",
    "\n",
    "\n",
    "# selected_features()\n",
    "\n",
    "def data_in_date_time():\n",
    "    \"\"\"This function would return the selected features in datetime64 format\"\"\"\n",
    "    data = selected_features()\n",
    "    datetime_data = data.astype({'Date': 'datetime64'})\n",
    "    return datetime_data\n",
    "\n",
    "\n",
    "# data_in_date_time()\n",
    "\n",
    "\n",
    "def lag_plot():\n",
    "    \"\"\"The lag plot would show the auto-correlation between the price and the day/time.\n",
    "    The day with a decent auto-correlation would form the basis upon which the data would be merged.\n",
    "    Also, the lag plot is useful in searching for patterns like;\n",
    "    1. Randomness 2. Trends 3. Seasonality.\"\"\"\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    plt.suptitle('Lag Plots', fontsize=22)\n",
    "\n",
    "    plt.subplot(3, 3, 1)\n",
    "    pd.plotting.lag_plot(data_in_date_time()['Close'], lag=1)\n",
    "    plt.title('1-Minute Lag')\n",
    "\n",
    "    plt.subplot(3, 3, 2)\n",
    "    pd.plotting.lag_plot(data_in_date_time()['Close'], lag=60)\n",
    "    plt.title('1-Hour Lag')\n",
    "\n",
    "    plt.subplot(3, 3, 3)\n",
    "    pd.plotting.lag_plot(data_in_date_time()['Close'], lag=1440)\n",
    "    plt.title('Daily Lag')\n",
    "\n",
    "    # plt.subplot(3, 3, 4)\n",
    "    # pd.plotting.lag_plot(data_in_date_time()['Close'], lag=10080)\n",
    "    # plt.title('Weekly Lag')\n",
    "\n",
    "    # plt.subplot(3, 3, 5)\n",
    "    # pd.plotting.lag_plot(data_in_date_time()['Close'], lag=43200)\n",
    "    # plt.title('1-Month Lag')\n",
    "\n",
    "    plt.legend('Lag Plots')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#lag_plot()\n",
    "\n",
    "crypto_symbols = ['BTC', 'ETH', 'LTC', 'FIL', 'BNB', 'SOL', 'AVAX', 'BUSD', 'DOGE', 'CELO', 'DENT', 'ADA', 'SRM', 'BTG', 'ETC', 'EOS', 'XRP', 'XLA', 'BCH', 'XMR', 'BTS']\n",
    "\n",
    "def all_close_price():\n",
    "    data = data_in_date_time()\n",
    "\n",
    "    everything=[]\n",
    "    for coin in crypto_symbols:\n",
    "        all_coin = data[(data['symbol'] == coin) & (data['Close'])]\n",
    "        all_coin = np.array(all_coin['Close'])\n",
    "        everything.append(all_coin[:-28])\n",
    "    #print(len(everything),len(everything[0]))  # the number of coins and lenght of each coin\n",
    "    return everything\n",
    "#all_close_price()\n",
    "def unseen_close_price():\n",
    "    data = data_in_date_time()\n",
    "\n",
    "    everything=[]\n",
    "    for coin in crypto_symbols:\n",
    "        all_coin = data[(data['symbol'] == coin) & (data['Close'])]\n",
    "        all_coin = np.array(all_coin['Close'])\n",
    "        everything.append(all_coin[-28:])\n",
    "    #print(len(everything),len(everything[0]))  # the number of coins and lenght of each coin\n",
    "    return everything\n",
    "#unseen_close_price()\n",
    "def X_test_final():\n",
    "    data = data_in_date_time()\n",
    "\n",
    "    everything=[]\n",
    "    for coin in crypto_symbols:\n",
    "        all_coin = data[(data['symbol'] == coin) & (data['Close'])]\n",
    "        all_coin = np.array(all_coin['Close'])\n",
    "        everything.append(all_coin[-63:-28])\n",
    "    print(len(everything),len(everything[0]))  # the number of coins and lenght of each coin\n",
    "    print(everything)\n",
    "X_test_final()\n",
    "\n",
    "\n",
    "def eth_close_price():\n",
    "    data = data_in_date_time()\n",
    "    eth = data[(data['symbol'] == 'ETH') & (data['Close'])]\n",
    "    eth_close_data = eth[['Date', 'Close']]\n",
    "    return eth_close_data\n",
    "\n",
    "\n",
    "# eth_close_price()\n",
    "\n",
    "u = data_in_date_time()\n",
    "#print(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f04bc0",
   "metadata": {},
   "source": [
    "MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "456f14e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score\n",
    "from sklearn.metrics import mean_poisson_deviance, mean_gamma_deviance, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from itertools import product\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "#model_data = all_close_price()\n",
    "#model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05c2e6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate seaquence into samples\n",
    "from numpy import array \n",
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return array(X), array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d77972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 35\n",
      "[array([37917.6015625 , 38483.125     , 38743.2734375 , 36952.984375  ,\n",
      "       37154.6015625 , 41500.875     , 41441.1640625 , 42412.43359375,\n",
      "       43840.28515625, 44118.4453125 , 44338.796875  , 43565.11328125,\n",
      "       42407.9375    , 42244.46875   , 42197.515625  , 42586.91796875,\n",
      "       44575.203125  , 43961.859375  , 40538.01171875, 40030.9765625 ,\n",
      "       40122.15625   , 38431.37890625, 37075.28125   , 38286.02734375,\n",
      "       37296.5703125 , 38332.609375  , 39214.21875   , 39105.1484375 ,\n",
      "       37709.78515625, 43193.234375  , 44354.63671875, 43924.1171875 ,\n",
      "       42451.7890625 , 39137.60546875, 39400.5859375 ]), array([2603.46655273, 2688.27880859, 2792.1171875 , 2682.85400391,\n",
      "       2679.16259766, 2983.58691406, 3014.64819336, 3057.47607422,\n",
      "       3142.47070312, 3122.60864258, 3239.45703125, 3077.48217773,\n",
      "       2927.38354492, 2917.36279297, 2883.46337891, 2933.47900391,\n",
      "       3179.87719727, 3127.83007812, 2881.48193359, 2785.72753906,\n",
      "       2763.70117188, 2628.6484375 , 2573.81616211, 2639.29931641,\n",
      "       2590.35961914, 2598.06713867, 2764.53564453, 2781.11181641,\n",
      "       2621.80175781, 2919.20117188, 2972.48510742, 2950.1184082 ,\n",
      "       2834.46899414, 2617.15600586, 2664.83105469]), array([108.79985809, 109.58769989, 115.40200043, 108.55602264,\n",
      "       110.28700256, 120.91803741, 121.91674042, 127.44550323,\n",
      "       137.10739136, 133.99124146, 140.17160034, 135.08839417,\n",
      "       126.03115845, 126.68367767, 126.204216  , 124.80005646,\n",
      "       131.76261902, 128.1608429 , 116.30241394, 115.19709015,\n",
      "       115.70900726, 110.83203888, 103.45049286, 107.52279663,\n",
      "       105.96595001, 104.23049164, 109.26631165, 108.24365997,\n",
      "       103.10559845, 113.47457886, 112.54404449, 110.35135651,\n",
      "       111.35211945, 101.39179993, 104.97718811]), array([19.83703041, 21.32290459, 20.91481209, 20.18340111, 20.31682205,\n",
      "       21.61750221, 22.01656914, 22.44775009, 24.40475464, 24.59631348,\n",
      "       24.26986122, 23.17950249, 21.03780174, 20.85038185, 20.66428566,\n",
      "       20.72755051, 22.34214783, 23.08382607, 20.48183632, 21.16025925,\n",
      "       20.87793732, 19.8388176 , 18.60088539, 19.21353149, 18.48812294,\n",
      "       18.42663574, 18.92191887, 19.07542038, 20.34723663, 21.67240715,\n",
      "       20.9356308 , 20.68394661, 20.14182663, 18.77859497, 19.10863876]), array([377.61291504, 375.27731323, 385.46380615, 367.47167969,\n",
      "       370.87774658, 399.02960205, 414.20001221, 419.54556274,\n",
      "       433.22927856, 408.77676392, 422.46554565, 415.11923218,\n",
      "       400.4223938 , 403.38973999, 398.88677979, 403.66079712,\n",
      "       432.40939331, 427.91812134, 402.45004272, 399.56918335,\n",
      "       400.0435791 , 380.82592773, 356.66125488, 374.3074646 ,\n",
      "       366.53948975, 361.23464966, 374.91494751, 373.63876343,\n",
      "       360.53103638, 395.61242676, 408.4822998 , 409.00030518,\n",
      "       402.5506897 , 374.28088379, 384.87335205]), array([ 93.29767609,  99.7388382 , 109.97898102, 101.83760071,\n",
      "       101.45903778, 112.17424774, 113.89430237, 115.29975891,\n",
      "       117.35514832, 113.59548187, 114.01937866, 106.20280457,\n",
      "        96.38188171,  95.91820526,  93.24410248,  96.43327332,\n",
      "       104.87330627, 101.93289948,  93.41042328,  90.06546021,\n",
      "        91.4966507 ,  90.90141296,  83.12316132,  86.46538544,\n",
      "        84.90278625,  89.19474792,  92.59882355,  90.14156342,\n",
      "        85.5190506 ,  99.5203476 ,  98.65355682, 100.64963531,\n",
      "        95.53919983,  88.24597931,  89.64784241]), array([67.9737854 , 69.99242401, 73.44881439, 67.87997437, 68.10771942,\n",
      "       77.31163788, 77.24948883, 78.75726318, 83.16699982, 86.62419891,\n",
      "       90.02086639, 91.13622284, 81.73299408, 81.24567413, 78.24311829,\n",
      "       82.01966858, 92.67250061, 95.34611511, 88.12377167, 83.8369751 ,\n",
      "       85.09931946, 77.33678436, 69.98892975, 74.57435608, 73.69863129,\n",
      "       76.52396393, 80.83261871, 81.87970734, 74.99118805, 84.3392334 ,\n",
      "       86.7359314 , 83.29166412, 78.55764771, 75.65424347, 76.72742462]), array([1.00098205, 1.00010002, 1.00088096, 0.998694  , 1.00127697,\n",
      "       0.99969298, 0.99883997, 0.99989802, 0.99927598, 1.00058305,\n",
      "       1.00034201, 0.99893802, 0.99895102, 0.99962002, 0.99866599,\n",
      "       0.998698  , 1.00044799, 0.999201  , 1.00060797, 0.99906099,\n",
      "       0.99979001, 0.99885303, 0.99849403, 1.00072801, 0.99973702,\n",
      "       0.99977797, 1.00057995, 1.000422  , 0.99922299, 1.00068998,\n",
      "       1.00033104, 0.999089  , 1.00072503, 1.00031495, 0.99977601]), array([0.13947099, 0.14180499, 0.14263099, 0.137235  , 0.137541  ,\n",
      "       0.147503  , 0.14714099, 0.15377   , 0.16557799, 0.15840501,\n",
      "       0.159123  , 0.151889  , 0.14484701, 0.14440501, 0.148948  ,\n",
      "       0.14600299, 0.151761  , 0.149095  , 0.138552  , 0.138768  ,\n",
      "       0.141206  , 0.136868  , 0.12849   , 0.13155299, 0.127846  ,\n",
      "       0.123813  , 0.12757599, 0.127647  , 0.123111  , 0.133156  ,\n",
      "       0.133938  , 0.13299   , 0.12961   , 0.122591  , 0.124996  ]), array([3.03029203, 3.16999292, 3.23397708, 3.02824211, 3.02578211,\n",
      "       3.2406311 , 3.24063492, 3.36664796, 3.52817702, 3.31245589,\n",
      "       3.44672203, 3.13077307, 3.05570197, 3.05371094, 2.94332409,\n",
      "       2.90310097, 3.17121506, 3.01136804, 2.83376002, 2.74520397,\n",
      "       2.72262192, 2.56742811, 2.4122591 , 2.52946091, 2.38801599,\n",
      "       2.2785759 , 2.39982104, 2.41350508, 2.36145711, 2.64371204,\n",
      "       2.67795491, 2.58418989, 2.55623507, 2.65787792, 2.854357  ]), array([0.002512, 0.002627, 0.002626, 0.002505, 0.002501, 0.002801,\n",
      "       0.002942, 0.003059, 0.003268, 0.003307, 0.003328, 0.003129,\n",
      "       0.002899, 0.002908, 0.002804, 0.002836, 0.00311 , 0.003035,\n",
      "       0.002758, 0.002663, 0.002685, 0.002552, 0.002334, 0.002402,\n",
      "       0.002335, 0.002279, 0.002404, 0.00238 , 0.002293, 0.002639,\n",
      "       0.002624, 0.002527, 0.002512, 0.002344, 0.00245 ]), array([1.03747296, 1.05230296, 1.088624  , 1.02890599, 1.05644405,\n",
      "       1.13562405, 1.12734497, 1.14454699, 1.19730699, 1.17872   ,\n",
      "       1.19481003, 1.15247798, 1.08034694, 1.05819905, 1.04581904,\n",
      "       1.05026698, 1.10758603, 1.08498096, 1.01989496, 0.99711299,\n",
      "       0.99770302, 0.93290198, 0.85823601, 0.88901699, 0.869169  ,\n",
      "       0.85342503, 0.898027  , 0.887918  , 0.85833502, 0.95982802,\n",
      "       0.96234602, 0.93866199, 0.902659  , 0.84288901, 0.86567199]), array([2.30935907, 2.31433105, 2.36847901, 2.25828695, 2.25151801,\n",
      "       2.42736602, 2.531353  , 2.55887294, 2.68890595, 2.55821705,\n",
      "       2.64385605, 2.48662305, 2.33125091, 2.27966499, 2.25969195,\n",
      "       2.26875305, 2.45159197, 2.43607807, 2.207582  , 2.16115189,\n",
      "       2.15277004, 2.06267095, 1.91123497, 1.95113504, 1.88136399,\n",
      "       1.83923697, 1.94465601, 1.97995603, 1.90357006, 2.08884597,\n",
      "       2.09130192, 2.07455492, 1.97607601, 1.83452404, 1.87580204]), array([29.61634827, 29.62545395, 29.91239929, 29.50370026, 29.65930557,\n",
      "       32.50956726, 33.07117462, 34.09070587, 36.14395523, 35.20979691,\n",
      "       36.17395782, 35.9116478 , 33.62769318, 33.00115585, 32.96675491,\n",
      "       32.54729462, 34.26016617, 33.87879944, 31.50341415, 30.80136108,\n",
      "       30.23965073, 29.16626549, 27.48212624, 28.1492939 , 28.04890442,\n",
      "       27.73696327, 28.97377777, 30.58506966, 29.25064659, 31.90309525,\n",
      "       32.69284058, 31.55937576, 31.11388779, 29.91573524, 29.87197113]), array([25.01261711, 25.71295547, 26.82476234, 26.31565857, 27.90536308,\n",
      "       29.69120407, 29.43660736, 29.8921299 , 32.6400528 , 32.09848785,\n",
      "       33.83422089, 34.45188522, 32.69767761, 31.6366272 , 32.6863327 ,\n",
      "       31.39624977, 33.47097778, 32.30659103, 29.39853287, 29.04769707,\n",
      "       28.76227951, 27.41193581, 25.03525352, 26.69472504, 26.5665226 ,\n",
      "       26.58348083, 28.02692032, 28.88312912, 27.37037086, 30.1151371 ,\n",
      "       30.31150055, 29.53998756, 30.21997452, 27.5830307 , 27.83560181]), array([2.31907892, 2.34201694, 2.35694909, 2.2803359 , 2.29807711,\n",
      "       2.48021293, 2.48028708, 2.54000306, 2.68913794, 2.63366199,\n",
      "       2.6693399 , 2.56111908, 2.41559792, 2.39493299, 2.38979506,\n",
      "       2.37290692, 2.52687812, 2.55372691, 2.32525992, 2.29921699,\n",
      "       2.28920293, 2.20611   , 2.08092403, 2.16894007, 2.11076498,\n",
      "       2.06846809, 2.14414406, 2.16137695, 2.0928781 , 2.2793951 ,\n",
      "       2.26443291, 2.21259594, 2.15981889, 2.00251102, 2.04351711]), array([0.603181  , 0.61914903, 0.629233  , 0.60148299, 0.60768402,\n",
      "       0.65587699, 0.66712701, 0.68364698, 0.82781899, 0.87819397,\n",
      "       0.87062699, 0.83001   , 0.76022601, 0.82240802, 0.81027299,\n",
      "       0.80221802, 0.85233301, 0.83903599, 0.76717299, 0.78588998,\n",
      "       0.822528  , 0.77829599, 0.703466  , 0.72283298, 0.69874603,\n",
      "       0.69627202, 0.76890302, 0.75097299, 0.72199202, 0.78204399,\n",
      "       0.78486198, 0.76849598, 0.75218499, 0.712672  , 0.75489998]), array([9.50000031e-05, 9.99999975e-05, 1.01999998e-04, 9.40000027e-05,\n",
      "       9.50000031e-05, 1.01999998e-04, 9.79999968e-05, 9.60000034e-05,\n",
      "       1.04999999e-04, 9.89999971e-05, 1.09000001e-04, 9.79999968e-05,\n",
      "       8.70000003e-05, 9.00000014e-05, 9.20000020e-05, 8.29999990e-05,\n",
      "       8.49999997e-05, 1.08000000e-04, 9.40000027e-05, 9.30000024e-05,\n",
      "       9.50000031e-05, 7.99999980e-05, 7.59999966e-05, 7.40000032e-05,\n",
      "       7.30000029e-05, 7.20000025e-05, 7.59999966e-05, 7.69999970e-05,\n",
      "       7.20000025e-05, 8.29999990e-05, 7.79999973e-05, 7.89999976e-05,\n",
      "       7.89999976e-05, 7.20000025e-05, 7.69999970e-05]), array([290.50073242, 285.53927612, 288.1484375 , 278.59036255,\n",
      "       279.97592163, 305.98312378, 320.87854004, 325.0093689 ,\n",
      "       342.79690552, 336.25402832, 345.83364868, 341.83203125,\n",
      "       320.55270386, 329.23687744, 334.87149048, 331.33441162,\n",
      "       343.13092041, 335.36160278, 312.81619263, 310.97793579,\n",
      "       312.02392578, 303.13562012, 284.34567261, 293.675354  ,\n",
      "       289.30834961, 300.81051636, 308.45904541, 314.53085327,\n",
      "       304.10360718, 337.12200928, 328.23138428, 320.40515137,\n",
      "       315.12631226, 289.91744995, 293.9798584 ]), array([147.91908264, 147.37472534, 146.16348267, 144.80357361,\n",
      "       146.63528442, 165.12031555, 168.68193054, 173.52580261,\n",
      "       184.63526917, 178.89982605, 185.62585449, 179.44432068,\n",
      "       168.67033386, 173.77044678, 174.39193726, 176.29377747,\n",
      "       182.37001038, 179.30949402, 163.38528442, 160.13787842,\n",
      "       161.28518677, 153.93527222, 147.96923828, 150.90545654,\n",
      "       149.12062073, 147.03302002, 153.46913147, 157.66094971,\n",
      "       150.72886658, 170.19030762, 177.53414917, 177.70994568,\n",
      "       172.03263855, 159.71348572, 165.77366638]), array([0.023964, 0.023977, 0.024595, 0.023239, 0.022525, 0.025045,\n",
      "       0.026203, 0.027182, 0.02802 , 0.027554, 0.027915, 0.026407,\n",
      "       0.024564, 0.024278, 0.023518, 0.023734, 0.025834, 0.025314,\n",
      "       0.022957, 0.022646, 0.022386, 0.021422, 0.019739, 0.020452,\n",
      "       0.019704, 0.019932, 0.02128 , 0.020597, 0.01957 , 0.021978,\n",
      "       0.023013, 0.02256 , 0.021555, 0.020155, 0.020273])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = all_close_price()  \n",
    "\n",
    "dataset=np.hstack((data[i].reshape((len(data[i]),1)) for i in range(len(data))))\n",
    " \n",
    "X,y=split_sequences(dataset, 35, 7)\n",
    "\n",
    "# summarize the data\n",
    "#for i in range(len(X)):\n",
    "    #print(X[i], y[i])\n",
    "    \n",
    "    \n",
    "#1,2,3 4 5....35 = 30,31,32,33,34,35   36, 37\n",
    "#1. 1,2,3,4,5,6    7,8,9,10\n",
    "#2. 2,3,4,5,6,7    8,9,10,11\n",
    "#3. 3,4,5,6,7,8   9,10,11,12\n",
    "#3, 4, 5,4...25\n",
    "#(357, 7,1, 21)\n",
    "y_actual_final=unseen_close_price()\n",
    "X_test_final=X_test_final()\n",
    "np.shape(X_test_final)\n",
    "\n",
    "# 1-100\n",
    "# 70-100\n",
    "\n",
    "# 35-70 -   70-77\n",
    "# 35-77\n",
    "# 42-77 -  77-84\n",
    "# 49-84    84-91\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "935e7919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "\n",
    "# multivariate multi-step encoder-decoder lstm example\n",
    "from numpy import array\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "import tensorflow as tf\n",
    "n_features =  X.shape[2]\n",
    "n_steps_in=35\n",
    "n_steps_out=7\n",
    "#dropout = 0.4\n",
    "model = Sequential()\n",
    "model.add(LSTM(200, activation='relu', input_shape=(35, n_features)))\n",
    "\n",
    "model.add(RepeatVector(n_steps_out))\n",
    "model.add(LSTM(200, activation='relu', return_sequences=True))\n",
    "model.add(TimeDistributed(Dense(n_features*8)))\n",
    "model.add(TimeDistributed(Dense(n_features*4)))\n",
    "model.add(TimeDistributed(Dense(n_features*2)))\n",
    "model.add(TimeDistributed(Dense(n_features)))\n",
    "#tf.keras.layers.Dropout(0.7, n_features)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "#X.shape\n",
    "\n",
    "print(n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86ff8c1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "10/10 [==============================] - 5s 172ms/step - loss: 537013632.0000 - val_loss: 219711600.0000\n",
      "Epoch 2/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 105252760.0000 - val_loss: 113007896.0000\n",
      "Epoch 3/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 60255492.0000 - val_loss: 94269624.0000\n",
      "Epoch 4/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 58801960.0000 - val_loss: 68454664.0000\n",
      "Epoch 5/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 42828484.0000 - val_loss: 50436400.0000\n",
      "Epoch 6/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 31330924.0000 - val_loss: 40046920.0000\n",
      "Epoch 7/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 27104588.0000 - val_loss: 39648708.0000\n",
      "Epoch 8/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 32796882.0000 - val_loss: 51465620.0000\n",
      "Epoch 9/500\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 29788854.0000 - val_loss: 41197540.0000\n",
      "Epoch 10/500\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 29051116.0000 - val_loss: 44081572.0000\n",
      "Epoch 11/500\n",
      "10/10 [==============================] - 1s 70ms/step - loss: 29533808.0000 - val_loss: 44325800.0000\n",
      "Epoch 12/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 24277668.0000 - val_loss: 42361388.0000\n",
      "Epoch 13/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 24497210.0000 - val_loss: 68048840.0000\n",
      "Epoch 14/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 23168244.0000 - val_loss: 31742544.0000\n",
      "Epoch 15/500\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 27297182.0000 - val_loss: 47326856.0000\n",
      "Epoch 16/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 21956752.0000 - val_loss: 30655114.0000\n",
      "Epoch 17/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 20341886.0000 - val_loss: 35991884.0000\n",
      "Epoch 18/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 22341750.0000 - val_loss: 31400522.0000\n",
      "Epoch 19/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 18681168.0000 - val_loss: 23209742.0000\n",
      "Epoch 20/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 17279508.0000 - val_loss: 24347366.0000\n",
      "Epoch 21/500\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 18158088.0000 - val_loss: 24905566.0000\n",
      "Epoch 22/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 21804200.0000 - val_loss: 33787672.0000\n",
      "Epoch 23/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 23662908.0000 - val_loss: 59962116.0000\n",
      "Epoch 24/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 28195436.0000 - val_loss: 56490432.0000\n",
      "Epoch 25/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 25740696.0000 - val_loss: 31359080.0000\n",
      "Epoch 26/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 19604658.0000 - val_loss: 18256706.0000\n",
      "Epoch 27/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 16551826.0000 - val_loss: 29114436.0000\n",
      "Epoch 28/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 23054890.0000 - val_loss: 38907652.0000\n",
      "Epoch 29/500\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 27433880.0000 - val_loss: 34289084.0000\n",
      "Epoch 30/500\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 25270212.0000 - val_loss: 22760270.0000\n",
      "Epoch 31/500\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 18999416.0000 - val_loss: 39524940.0000\n",
      "Epoch 32/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 16791384.0000 - val_loss: 15090872.0000\n",
      "Epoch 33/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 13850935.0000 - val_loss: 26879298.0000\n",
      "Epoch 34/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 16798812.0000 - val_loss: 19172864.0000\n",
      "Epoch 35/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 15638049.0000 - val_loss: 21146050.0000\n",
      "Epoch 36/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 12924428.0000 - val_loss: 12852441.0000\n",
      "Epoch 37/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 18603694.0000 - val_loss: 24560688.0000\n",
      "Epoch 38/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 17436398.0000 - val_loss: 33669868.0000\n",
      "Epoch 39/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 18384040.0000 - val_loss: 27773420.0000\n",
      "Epoch 40/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 24315550.0000 - val_loss: 28193348.0000\n",
      "Epoch 41/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 25720976.0000 - val_loss: 40683756.0000\n",
      "Epoch 42/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 24147804.0000 - val_loss: 20166522.0000\n",
      "Epoch 43/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 12278999.0000 - val_loss: 18532768.0000\n",
      "Epoch 44/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 8655805.0000 - val_loss: 7695828.0000\n",
      "Epoch 45/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 5651463.5000 - val_loss: 4947292.5000\n",
      "Epoch 46/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 7146704.0000 - val_loss: 4134837.2500\n",
      "Epoch 47/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 9509408.0000 - val_loss: 19731020.0000\n",
      "Epoch 48/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 12499685.0000 - val_loss: 10428916.0000\n",
      "Epoch 49/500\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 12422276.0000 - val_loss: 30886300.0000\n",
      "Epoch 50/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 15610021.0000 - val_loss: 8588811.0000\n",
      "Epoch 51/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 8973335.0000 - val_loss: 16110920.0000\n",
      "Epoch 52/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 8079137.5000 - val_loss: 8826587.0000\n",
      "Epoch 53/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 7375396.5000 - val_loss: 4490172.5000\n",
      "Epoch 54/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 4715669.0000 - val_loss: 9748398.0000\n",
      "Epoch 55/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 4864456.0000 - val_loss: 4423027.5000\n",
      "Epoch 56/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 6070454.0000 - val_loss: 17320282.0000\n",
      "Epoch 57/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 9318490.0000 - val_loss: 6676400.5000\n",
      "Epoch 58/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 8544716.0000 - val_loss: 6344053.0000\n",
      "Epoch 59/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 7929862.5000 - val_loss: 9505124.0000\n",
      "Epoch 60/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 7594555.0000 - val_loss: 6244485.0000\n",
      "Epoch 61/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 22642406.0000 - val_loss: 27481860.0000\n",
      "Epoch 62/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 18819962.0000 - val_loss: 15833170.0000\n",
      "Epoch 63/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 25812704.0000 - val_loss: 25419650.0000\n",
      "Epoch 64/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 17912316.0000 - val_loss: 12164319.0000\n",
      "Epoch 65/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 19618070.0000 - val_loss: 26982368.0000\n",
      "Epoch 66/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 17697250.0000 - val_loss: 14591300.0000\n",
      "Epoch 67/500\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 7978626.0000 - val_loss: 4996035.5000\n",
      "Epoch 68/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 7225744.5000 - val_loss: 5657157.0000\n",
      "Epoch 69/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 4805098.0000 - val_loss: 5522612.0000\n",
      "Epoch 70/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 3780912.2500 - val_loss: 2396031.7500\n",
      "Epoch 71/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 3860611.0000 - val_loss: 3144983.0000\n",
      "Epoch 72/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 86ms/step - loss: 4015687.2500 - val_loss: 2996497.0000\n",
      "Epoch 73/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 5441930.0000 - val_loss: 4419668.0000\n",
      "Epoch 74/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 4363758.5000 - val_loss: 3458445.2500\n",
      "Epoch 75/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 3847286.5000 - val_loss: 2619364.5000\n",
      "Epoch 76/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 3701149.2500 - val_loss: 2330336.7500\n",
      "Epoch 77/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 3473937.7500 - val_loss: 2525373.0000\n",
      "Epoch 78/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 3337089.0000 - val_loss: 2819297.2500\n",
      "Epoch 79/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 3514835.5000 - val_loss: 2658977.5000\n",
      "Epoch 80/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 3475778.2500 - val_loss: 1869713.7500\n",
      "Epoch 81/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 3500992.2500 - val_loss: 1454639.8750\n",
      "Epoch 82/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 3802177.0000 - val_loss: 3343306.7500\n",
      "Epoch 83/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 3459901.7500 - val_loss: 6591441.0000\n",
      "Epoch 84/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 3445497.2500 - val_loss: 2881148.5000\n",
      "Epoch 85/500\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 3038859.2500 - val_loss: 3097717.2500\n",
      "Epoch 86/500\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 3022072.7500 - val_loss: 2481232.0000\n",
      "Epoch 87/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 3169324.7500 - val_loss: 3546007.5000\n",
      "Epoch 88/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 2947650.7500 - val_loss: 2651985.5000\n",
      "Epoch 89/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 3072469.0000 - val_loss: 5534222.5000\n",
      "Epoch 90/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 3123366.7500 - val_loss: 2599926.2500\n",
      "Epoch 91/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 3221234.5000 - val_loss: 3232595.2500\n",
      "Epoch 92/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 3288737.7500 - val_loss: 2334876.5000\n",
      "Epoch 93/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 2822899.2500 - val_loss: 2068446.5000\n",
      "Epoch 94/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 2874698.0000 - val_loss: 4262563.5000\n",
      "Epoch 95/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 2522731.0000 - val_loss: 2829391.5000\n",
      "Epoch 96/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 2903424.2500 - val_loss: 1792268.6250\n",
      "Epoch 97/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 2869417.5000 - val_loss: 2289695.5000\n",
      "Epoch 98/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 2786216.5000 - val_loss: 2321390.0000\n",
      "Epoch 99/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 2676026.0000 - val_loss: 2188825.0000\n",
      "Epoch 100/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 2531539.5000 - val_loss: 1515017.0000\n",
      "Epoch 101/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 3332197.5000 - val_loss: 1604321.2500\n",
      "Epoch 102/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 2626497.2500 - val_loss: 1629373.3750\n",
      "Epoch 103/500\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 2955156.0000 - val_loss: 9192933.0000\n",
      "Epoch 104/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 3731157.2500 - val_loss: 2907727.5000\n",
      "Epoch 105/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 3034397.5000 - val_loss: 3345900.0000\n",
      "Epoch 106/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 3080043.2500 - val_loss: 2564335.0000\n",
      "Epoch 107/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 3048850.0000 - val_loss: 2967377.0000\n",
      "Epoch 108/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 2965597.7500 - val_loss: 2191178.5000\n",
      "Epoch 109/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 2966351.0000 - val_loss: 2915509.7500\n",
      "Epoch 110/500\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 2839989.7500 - val_loss: 5573914.5000\n",
      "Epoch 111/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 2843201.5000 - val_loss: 2167017.2500\n",
      "Epoch 112/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 3060961.7500 - val_loss: 1699064.7500\n",
      "Epoch 113/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 3493522.5000 - val_loss: 3218887.0000\n",
      "Epoch 114/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 3120205.7500 - val_loss: 2376103.2500\n",
      "Epoch 115/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 3068161.2500 - val_loss: 2020057.0000\n",
      "Epoch 116/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 3451334.5000 - val_loss: 2981521.0000\n",
      "Epoch 117/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 2987635.2500 - val_loss: 2196733.5000\n",
      "Epoch 118/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 3082025.7500 - val_loss: 1773391.5000\n",
      "Epoch 119/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 4065463.2500 - val_loss: 3212133.5000\n",
      "Epoch 120/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 2848851.0000 - val_loss: 1995397.3750\n",
      "Epoch 121/500\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 2854131.0000 - val_loss: 1998764.8750\n",
      "Epoch 122/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 2997663.0000 - val_loss: 3273265.5000\n",
      "Epoch 123/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 2879251.0000 - val_loss: 5209544.5000\n",
      "Epoch 124/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 3289352.2500 - val_loss: 1487550.5000\n",
      "Epoch 125/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 3202713.0000 - val_loss: 1564790.3750\n",
      "Epoch 126/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 2867008.2500 - val_loss: 4332937.0000\n",
      "Epoch 127/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 2974011.0000 - val_loss: 4940917.0000\n",
      "Epoch 128/500\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 2740431.5000 - val_loss: 5052150.5000\n",
      "Epoch 129/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 2635515.0000 - val_loss: 3848565.7500\n",
      "Epoch 130/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 2711599.2500 - val_loss: 4788469.5000\n",
      "Epoch 131/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 2784402.0000 - val_loss: 3945975.7500\n",
      "Epoch 132/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 3500512.0000 - val_loss: 8851662.0000\n",
      "Epoch 133/500\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 5325542.0000 - val_loss: 4469197.5000\n",
      "Epoch 134/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 4633194.0000 - val_loss: 2758338.0000\n",
      "Epoch 135/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 4047677.7500 - val_loss: 5005287.5000\n",
      "Epoch 136/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 3063966.5000 - val_loss: 3217776.0000\n",
      "Epoch 137/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 3013794.5000 - val_loss: 6359764.5000\n",
      "Epoch 138/500\n",
      "10/10 [==============================] - 1s 73ms/step - loss: 2928607.5000 - val_loss: 4496264.5000\n",
      "Epoch 139/500\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 2858140.7500 - val_loss: 3471729.5000\n",
      "Epoch 140/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 2543803.0000 - val_loss: 2083486.5000\n",
      "Epoch 141/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 2571868.2500 - val_loss: 4060489.7500\n",
      "Epoch 142/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 2346008.0000 - val_loss: 1219816.6250\n",
      "Epoch 143/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 2583251.7500 - val_loss: 5560691.5000\n",
      "Epoch 144/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 88ms/step - loss: 2265365.2500 - val_loss: 2342419.5000\n",
      "Epoch 145/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 2311848.0000 - val_loss: 3882492.0000\n",
      "Epoch 146/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 2545174.7500 - val_loss: 5156139.0000\n",
      "Epoch 147/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 2334066.2500 - val_loss: 1783258.2500\n",
      "Epoch 148/500\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 2701133.0000 - val_loss: 2313353.7500\n",
      "Epoch 149/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 2756827.5000 - val_loss: 4544730.5000\n",
      "Epoch 150/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 2786188.0000 - val_loss: 1996383.7500\n",
      "Epoch 151/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 2920955.7500 - val_loss: 1750607.7500\n",
      "Epoch 152/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 2451395.7500 - val_loss: 6526969.5000\n",
      "Epoch 153/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 2992799.0000 - val_loss: 3560547.5000\n",
      "Epoch 154/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 2348504.7500 - val_loss: 2297418.5000\n",
      "Epoch 155/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 2359706.0000 - val_loss: 3063297.0000\n",
      "Epoch 156/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 2934787.0000 - val_loss: 6232836.0000\n",
      "Epoch 157/500\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 2498680.0000 - val_loss: 3176444.5000\n",
      "Epoch 158/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 2448071.2500 - val_loss: 2588425.7500\n",
      "Epoch 159/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 3093508.2500 - val_loss: 10342924.0000\n",
      "Epoch 160/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 3852432.5000 - val_loss: 10919282.0000\n",
      "Epoch 161/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 3194072.0000 - val_loss: 10659688.0000\n",
      "Epoch 162/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 3010971.5000 - val_loss: 5113227.0000\n",
      "Epoch 163/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 2306674.5000 - val_loss: 2382948.0000\n",
      "Epoch 164/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 2192644.2500 - val_loss: 1752217.0000\n",
      "Epoch 165/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 2313137.7500 - val_loss: 1420996.5000\n",
      "Epoch 166/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 2677057.7500 - val_loss: 2145505.0000\n",
      "Epoch 167/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 2314807.2500 - val_loss: 3601809.5000\n",
      "Epoch 168/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 2495531.0000 - val_loss: 4573653.5000\n",
      "Epoch 169/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 2386256.5000 - val_loss: 1373225.5000\n",
      "Epoch 170/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 2565620.5000 - val_loss: 1465018.6250\n",
      "Epoch 171/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 3159779.7500 - val_loss: 3912026.0000\n",
      "Epoch 172/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 2484068.2500 - val_loss: 2490737.5000\n",
      "Epoch 173/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 2368507.2500 - val_loss: 2447496.2500\n",
      "Epoch 174/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 2367502.2500 - val_loss: 3377166.5000\n",
      "Epoch 175/500\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 2009939.2500 - val_loss: 4779244.0000\n",
      "Epoch 176/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 1977981.5000 - val_loss: 1053437.2500\n",
      "Epoch 177/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 2556043.5000 - val_loss: 987029.7500\n",
      "Epoch 178/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 2538184.7500 - val_loss: 1017764.8750\n",
      "Epoch 179/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 2271121.7500 - val_loss: 2217497.0000\n",
      "Epoch 180/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1924157.3750 - val_loss: 3133751.0000\n",
      "Epoch 181/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1687351.8750 - val_loss: 1290232.7500\n",
      "Epoch 182/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1718289.5000 - val_loss: 1857368.1250\n",
      "Epoch 183/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1713426.2500 - val_loss: 1320968.7500\n",
      "Epoch 184/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1480735.8750 - val_loss: 2548571.7500\n",
      "Epoch 185/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1744391.0000 - val_loss: 2271768.2500\n",
      "Epoch 186/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 1400258.6250 - val_loss: 4643392.0000\n",
      "Epoch 187/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1644204.3750 - val_loss: 6810694.0000\n",
      "Epoch 188/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1834867.7500 - val_loss: 4042517.7500\n",
      "Epoch 189/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1631867.2500 - val_loss: 3417639.2500\n",
      "Epoch 190/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 1291516.3750 - val_loss: 1765322.0000\n",
      "Epoch 191/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1426398.7500 - val_loss: 1556795.6250\n",
      "Epoch 192/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1358528.1250 - val_loss: 1833566.5000\n",
      "Epoch 193/500\n",
      "10/10 [==============================] - 1s 76ms/step - loss: 1956066.5000 - val_loss: 1282203.1250\n",
      "Epoch 194/500\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1529492.1250 - val_loss: 1737178.2500\n",
      "Epoch 195/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 2020048.6250 - val_loss: 8034523.0000\n",
      "Epoch 196/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1715504.0000 - val_loss: 2915458.0000\n",
      "Epoch 197/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1524001.1250 - val_loss: 861263.1250\n",
      "Epoch 198/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1660164.2500 - val_loss: 827671.5625\n",
      "Epoch 199/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1420697.2500 - val_loss: 807030.5000\n",
      "Epoch 200/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1256670.3750 - val_loss: 5868985.0000\n",
      "Epoch 201/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1449731.6250 - val_loss: 6954374.0000\n",
      "Epoch 202/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1755205.0000 - val_loss: 3432286.2500\n",
      "Epoch 203/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 1379974.8750 - val_loss: 2770521.2500\n",
      "Epoch 204/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1663677.7500 - val_loss: 857504.8750\n",
      "Epoch 205/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 1932164.6250 - val_loss: 818119.7500\n",
      "Epoch 206/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1542290.2500 - val_loss: 1474621.2500\n",
      "Epoch 207/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1207703.7500 - val_loss: 2990851.5000\n",
      "Epoch 208/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1243563.0000 - val_loss: 2068079.7500\n",
      "Epoch 209/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 1638357.6250 - val_loss: 2689802.2500\n",
      "Epoch 210/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1834739.6250 - val_loss: 2160144.2500\n",
      "Epoch 211/500\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 1501337.5000 - val_loss: 2211476.0000\n",
      "Epoch 212/500\n",
      "10/10 [==============================] - 1s 75ms/step - loss: 2069636.6250 - val_loss: 1596830.2500\n",
      "Epoch 213/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 9153428.0000 - val_loss: 7317109.0000\n",
      "Epoch 214/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 6886168.5000 - val_loss: 8738948.0000\n",
      "Epoch 215/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 12574837.0000 - val_loss: 4706213.5000\n",
      "Epoch 216/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 91ms/step - loss: 6617777.0000 - val_loss: 6047057.0000\n",
      "Epoch 217/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 5896531.0000 - val_loss: 3487913.7500\n",
      "Epoch 218/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 5426496.0000 - val_loss: 3003103.5000\n",
      "Epoch 219/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 4605008.0000 - val_loss: 2395174.0000\n",
      "Epoch 220/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 4486286.5000 - val_loss: 2228626.2500\n",
      "Epoch 221/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 5322897.5000 - val_loss: 5162888.0000\n",
      "Epoch 222/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 4920136.0000 - val_loss: 2144259.0000\n",
      "Epoch 223/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 4508579.5000 - val_loss: 3071853.2500\n",
      "Epoch 224/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 5026926.0000 - val_loss: 5309938.5000\n",
      "Epoch 225/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 4572528.0000 - val_loss: 3347070.2500\n",
      "Epoch 226/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 4062807.5000 - val_loss: 3023973.7500\n",
      "Epoch 227/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 5245428.5000 - val_loss: 3165895.7500\n",
      "Epoch 228/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 3708867.2500 - val_loss: 2273650.7500\n",
      "Epoch 229/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 4389094.5000 - val_loss: 3196162.5000\n",
      "Epoch 230/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 4758731.5000 - val_loss: 3746250.2500\n",
      "Epoch 231/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 4105839.5000 - val_loss: 4377201.5000\n",
      "Epoch 232/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 4490122.0000 - val_loss: 5207214.0000\n",
      "Epoch 233/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 4149740.0000 - val_loss: 2382387.2500\n",
      "Epoch 234/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 4204276.5000 - val_loss: 5908738.5000\n",
      "Epoch 235/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 3670957.7500 - val_loss: 2228976.0000\n",
      "Epoch 236/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 3839843.2500 - val_loss: 3811791.0000\n",
      "Epoch 237/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 3202232.0000 - val_loss: 4068995.5000\n",
      "Epoch 238/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 3572367.0000 - val_loss: 1947082.2500\n",
      "Epoch 239/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 4875028.5000 - val_loss: 4062181.5000\n",
      "Epoch 240/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 4622698.5000 - val_loss: 2558510.0000\n",
      "Epoch 241/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 3796839.0000 - val_loss: 2257618.5000\n",
      "Epoch 242/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 3463078.0000 - val_loss: 2577986.5000\n",
      "Epoch 243/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 3199256.7500 - val_loss: 2101050.0000\n",
      "Epoch 244/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 2890117.5000 - val_loss: 2274660.0000\n",
      "Epoch 245/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 2683463.5000 - val_loss: 4012367.0000\n",
      "Epoch 246/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 2720307.0000 - val_loss: 3334788.2500\n",
      "Epoch 247/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 2840495.7500 - val_loss: 1370537.0000\n",
      "Epoch 248/500\n",
      "10/10 [==============================] - 1s 79ms/step - loss: 3097489.2500 - val_loss: 1408793.7500\n",
      "Epoch 249/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 2618022.7500 - val_loss: 3153937.2500\n",
      "Epoch 250/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 2487505.7500 - val_loss: 5156627.5000\n",
      "Epoch 251/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 2532333.7500 - val_loss: 1926867.8750\n",
      "Epoch 252/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 2403180.0000 - val_loss: 5049922.5000\n",
      "Epoch 253/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 2440307.2500 - val_loss: 874711.1250\n",
      "Epoch 254/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 3091083.5000 - val_loss: 1167173.3750\n",
      "Epoch 255/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 3070208.0000 - val_loss: 7846395.0000\n",
      "Epoch 256/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 3154255.2500 - val_loss: 2651890.5000\n",
      "Epoch 257/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 2174566.5000 - val_loss: 2625970.5000\n",
      "Epoch 258/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 2020355.3750 - val_loss: 2655478.0000\n",
      "Epoch 259/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 2026437.6250 - val_loss: 1839357.1250\n",
      "Epoch 260/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 2016572.0000 - val_loss: 2434976.2500\n",
      "Epoch 261/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1741314.8750 - val_loss: 1253917.2500\n",
      "Epoch 262/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1762493.1250 - val_loss: 1982882.0000\n",
      "Epoch 263/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1794200.5000 - val_loss: 1884348.3750\n",
      "Epoch 264/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1798274.5000 - val_loss: 1387759.0000\n",
      "Epoch 265/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1605168.0000 - val_loss: 2920671.5000\n",
      "Epoch 266/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1764165.5000 - val_loss: 10820245.0000\n",
      "Epoch 267/500\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 10822384.0000 - val_loss: 9348404.0000\n",
      "Epoch 268/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 19221050.0000 - val_loss: 16678325.0000\n",
      "Epoch 269/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 14189390.0000 - val_loss: 15922333.0000\n",
      "Epoch 270/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 16597587.0000 - val_loss: 9058977.0000\n",
      "Epoch 271/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 6228150.0000 - val_loss: 4395515.5000\n",
      "Epoch 272/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 3467581.5000 - val_loss: 5953623.5000\n",
      "Epoch 273/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 5016731.5000 - val_loss: 9199177.0000\n",
      "Epoch 274/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 8251033.5000 - val_loss: 4266153.0000\n",
      "Epoch 275/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 4591850.5000 - val_loss: 2789697.7500\n",
      "Epoch 276/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 3676681.2500 - val_loss: 4851617.5000\n",
      "Epoch 277/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 4949286.5000 - val_loss: 3838199.5000\n",
      "Epoch 278/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 3094500.5000 - val_loss: 2861683.0000\n",
      "Epoch 279/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 3525339.0000 - val_loss: 3084975.0000\n",
      "Epoch 280/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 3417312.2500 - val_loss: 2715764.5000\n",
      "Epoch 281/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 2523234.5000 - val_loss: 1687274.1250\n",
      "Epoch 282/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 2141994.2500 - val_loss: 1495183.1250\n",
      "Epoch 283/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1955775.5000 - val_loss: 1366937.3750\n",
      "Epoch 284/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 1711334.8750 - val_loss: 1582622.7500\n",
      "Epoch 285/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1819959.6250 - val_loss: 2488818.0000\n",
      "Epoch 286/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1758384.0000 - val_loss: 2062790.0000\n",
      "Epoch 287/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 2127896.7500 - val_loss: 1073440.1250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 288/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 1960374.2500 - val_loss: 1044201.0625\n",
      "Epoch 289/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1668206.6250 - val_loss: 1423270.6250\n",
      "Epoch 290/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 1408475.1250 - val_loss: 1664521.7500\n",
      "Epoch 291/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 1375681.6250 - val_loss: 2739195.2500\n",
      "Epoch 292/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1287421.3750 - val_loss: 1526031.5000\n",
      "Epoch 293/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1236556.0000 - val_loss: 2637483.5000\n",
      "Epoch 294/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 1356119.6250 - val_loss: 695227.1250\n",
      "Epoch 295/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1234727.1250 - val_loss: 1024021.5625\n",
      "Epoch 296/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1291850.3750 - val_loss: 2581538.5000\n",
      "Epoch 297/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1416457.6250 - val_loss: 791862.5000\n",
      "Epoch 298/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1205736.1250 - val_loss: 1271406.5000\n",
      "Epoch 299/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1288826.6250 - val_loss: 1764279.7500\n",
      "Epoch 300/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 1274339.5000 - val_loss: 1184622.2500\n",
      "Epoch 301/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 1291349.7500 - val_loss: 1267650.8750\n",
      "Epoch 302/500\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1210039.6250 - val_loss: 708461.9375\n",
      "Epoch 303/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1334340.6250 - val_loss: 3073499.5000\n",
      "Epoch 304/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1143272.2500 - val_loss: 2742891.5000\n",
      "Epoch 305/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1102967.5000 - val_loss: 977190.3125\n",
      "Epoch 306/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1064622.1250 - val_loss: 1322722.3750\n",
      "Epoch 307/500\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1059970.5000 - val_loss: 1210370.3750\n",
      "Epoch 308/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1138481.2500 - val_loss: 1693745.7500\n",
      "Epoch 309/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 1097858.8750 - val_loss: 967598.8125\n",
      "Epoch 310/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1096997.1250 - val_loss: 882748.6250\n",
      "Epoch 311/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1197238.7500 - val_loss: 979215.7500\n",
      "Epoch 312/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1058065.2500 - val_loss: 603447.5000\n",
      "Epoch 313/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1055682.2500 - val_loss: 1042705.6875\n",
      "Epoch 314/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 1096296.5000 - val_loss: 682393.8750\n",
      "Epoch 315/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1198500.8750 - val_loss: 5420511.0000\n",
      "Epoch 316/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1136810.8750 - val_loss: 563867.6875\n",
      "Epoch 317/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1136386.2500 - val_loss: 1668201.6250\n",
      "Epoch 318/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 973643.7500 - val_loss: 1412225.6250\n",
      "Epoch 319/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 982675.0625 - val_loss: 1301883.0000\n",
      "Epoch 320/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1176534.2500 - val_loss: 3354093.2500\n",
      "Epoch 321/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1142380.1250 - val_loss: 1088049.6250\n",
      "Epoch 322/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 986588.7500 - val_loss: 3581828.7500\n",
      "Epoch 323/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 1162770.7500 - val_loss: 590908.0625\n",
      "Epoch 324/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 1130886.3750 - val_loss: 913467.6250\n",
      "Epoch 325/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 923732.8750 - val_loss: 2199940.0000\n",
      "Epoch 326/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 909457.8750 - val_loss: 594214.1250\n",
      "Epoch 327/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 1242519.7500 - val_loss: 1861838.7500\n",
      "Epoch 328/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1120890.7500 - val_loss: 709856.5000\n",
      "Epoch 329/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1077037.6250 - val_loss: 2022272.8750\n",
      "Epoch 330/500\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 883811.8125 - val_loss: 2274620.5000\n",
      "Epoch 331/500\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 896232.6250 - val_loss: 934900.3125\n",
      "Epoch 332/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 960629.0000 - val_loss: 3024781.2500\n",
      "Epoch 333/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 935504.6875 - val_loss: 555313.2500\n",
      "Epoch 334/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 914508.9375 - val_loss: 2342267.0000\n",
      "Epoch 335/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 910344.5625 - val_loss: 690758.5000\n",
      "Epoch 336/500\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 943176.8125 - val_loss: 1580252.2500\n",
      "Epoch 337/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 988817.3125 - val_loss: 1348983.5000\n",
      "Epoch 338/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 915547.0000 - val_loss: 1490023.8750\n",
      "Epoch 339/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 918130.3125 - val_loss: 4000963.0000\n",
      "Epoch 340/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1094500.1250 - val_loss: 1676974.7500\n",
      "Epoch 341/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1009639.4375 - val_loss: 795729.5000\n",
      "Epoch 342/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1168279.7500 - val_loss: 2989033.7500\n",
      "Epoch 343/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1203547.5000 - val_loss: 1375951.3750\n",
      "Epoch 344/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 1296717.1250 - val_loss: 830095.1250\n",
      "Epoch 345/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 969128.0000 - val_loss: 2200417.2500\n",
      "Epoch 346/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 898577.0000 - val_loss: 676535.1250\n",
      "Epoch 347/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 934036.9375 - val_loss: 1432969.0000\n",
      "Epoch 348/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 876424.6875 - val_loss: 815883.7500\n",
      "Epoch 349/500\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1008458.7500 - val_loss: 1394926.0000\n",
      "Epoch 350/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 880698.0625 - val_loss: 675519.1875\n",
      "Epoch 351/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 811680.2500 - val_loss: 734257.5000\n",
      "Epoch 352/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 864915.5000 - val_loss: 1344410.3750\n",
      "Epoch 353/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 883783.9375 - val_loss: 1118672.5000\n",
      "Epoch 354/500\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 1035822.8750 - val_loss: 497329.8125\n",
      "Epoch 355/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1042840.3125 - val_loss: 2043918.7500\n",
      "Epoch 356/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 908036.8750 - val_loss: 959510.8750\n",
      "Epoch 357/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1102426.2500 - val_loss: 1997215.8750\n",
      "Epoch 358/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1232554.0000 - val_loss: 1816343.2500\n",
      "Epoch 359/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1013392.3750 - val_loss: 850128.3125\n",
      "Epoch 360/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 98ms/step - loss: 1037414.9375 - val_loss: 2200532.7500\n",
      "Epoch 361/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1143927.3750 - val_loss: 496007.6875\n",
      "Epoch 362/500\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1091763.7500 - val_loss: 2122992.5000\n",
      "Epoch 363/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 941818.1875 - val_loss: 1284896.5000\n",
      "Epoch 364/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 937147.8750 - val_loss: 3002228.0000\n",
      "Epoch 365/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 935690.5625 - val_loss: 590523.1875\n",
      "Epoch 366/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1024623.7500 - val_loss: 1571421.3750\n",
      "Epoch 367/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 902190.3750 - val_loss: 1654743.8750\n",
      "Epoch 368/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 852152.5625 - val_loss: 1239775.1250\n",
      "Epoch 369/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 841135.3750 - val_loss: 1780020.6250\n",
      "Epoch 370/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 904931.5000 - val_loss: 634240.8750\n",
      "Epoch 371/500\n",
      "10/10 [==============================] - 1s 80ms/step - loss: 989984.7500 - val_loss: 751541.8750\n",
      "Epoch 372/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1055663.2500 - val_loss: 3710832.0000\n",
      "Epoch 373/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 1093519.0000 - val_loss: 659326.8125\n",
      "Epoch 374/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 912374.2500 - val_loss: 1091956.1250\n",
      "Epoch 375/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 811095.0625 - val_loss: 2470275.2500\n",
      "Epoch 376/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 838455.7500 - val_loss: 939136.3125\n",
      "Epoch 377/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 909939.1875 - val_loss: 792944.0625\n",
      "Epoch 378/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 851232.9375 - val_loss: 2931795.5000\n",
      "Epoch 379/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1012925.4375 - val_loss: 777819.0000\n",
      "Epoch 380/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1021345.8750 - val_loss: 678112.8125\n",
      "Epoch 381/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1234747.5000 - val_loss: 5923705.0000\n",
      "Epoch 382/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1493826.0000 - val_loss: 527110.6250\n",
      "Epoch 383/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 974676.1250 - val_loss: 1172424.5000\n",
      "Epoch 384/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 954448.6250 - val_loss: 2292389.5000\n",
      "Epoch 385/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 934516.5000 - val_loss: 2768548.7500\n",
      "Epoch 386/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 922048.4375 - val_loss: 1010329.4375\n",
      "Epoch 387/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 834781.1250 - val_loss: 688733.7500\n",
      "Epoch 388/500\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 856783.4375 - val_loss: 1525849.2500\n",
      "Epoch 389/500\n",
      "10/10 [==============================] - 1s 78ms/step - loss: 809559.4375 - val_loss: 1076396.6250\n",
      "Epoch 390/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 840011.1875 - val_loss: 921748.2500\n",
      "Epoch 391/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 967786.7500 - val_loss: 3069508.7500\n",
      "Epoch 392/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1122840.0000 - val_loss: 634545.6875\n",
      "Epoch 393/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 942227.0625 - val_loss: 653363.6250\n",
      "Epoch 394/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 1050014.6250 - val_loss: 3119854.7500\n",
      "Epoch 395/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1054260.7500 - val_loss: 749514.2500\n",
      "Epoch 396/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 904339.0000 - val_loss: 1933800.1250\n",
      "Epoch 397/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 801824.4375 - val_loss: 1344442.0000\n",
      "Epoch 398/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 763961.0625 - val_loss: 675676.8125\n",
      "Epoch 399/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 818967.3750 - val_loss: 2706147.5000\n",
      "Epoch 400/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 847724.4375 - val_loss: 663532.5000\n",
      "Epoch 401/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 827373.8750 - val_loss: 1356684.6250\n",
      "Epoch 402/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 909395.4375 - val_loss: 497657.3438\n",
      "Epoch 403/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 905104.3125 - val_loss: 1309742.6250\n",
      "Epoch 404/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 852552.6875 - val_loss: 2808985.7500\n",
      "Epoch 405/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 955642.5000 - val_loss: 974948.6250\n",
      "Epoch 406/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 1063404.6250 - val_loss: 719851.3125\n",
      "Epoch 407/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 943729.0000 - val_loss: 1885854.0000\n",
      "Epoch 408/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 856017.3750 - val_loss: 1252961.7500\n",
      "Epoch 409/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 794223.6875 - val_loss: 645653.1250\n",
      "Epoch 410/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1022433.6875 - val_loss: 846016.0000\n",
      "Epoch 411/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 823229.1250 - val_loss: 2417955.5000\n",
      "Epoch 412/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 896852.5000 - val_loss: 612317.8125\n",
      "Epoch 413/500\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 759633.6875 - val_loss: 812712.7500\n",
      "Epoch 414/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 757446.3125 - val_loss: 1415357.3750\n",
      "Epoch 415/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 783692.9375 - val_loss: 1503534.2500\n",
      "Epoch 416/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 726060.8125 - val_loss: 874796.6875\n",
      "Epoch 417/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 781350.3125 - val_loss: 1470342.6250\n",
      "Epoch 418/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 806944.1250 - val_loss: 1666258.3750\n",
      "Epoch 419/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 781545.4375 - val_loss: 1282124.2500\n",
      "Epoch 420/500\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 719820.7500 - val_loss: 1036938.5625\n",
      "Epoch 421/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 1913033.1250 - val_loss: 5403712.0000\n",
      "Epoch 422/500\n",
      "10/10 [==============================] - 1s 86ms/step - loss: 4132872.0000 - val_loss: 3545728.5000\n",
      "Epoch 423/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 3293097.2500 - val_loss: 2936637.5000\n",
      "Epoch 424/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 3217982.7500 - val_loss: 4224140.0000\n",
      "Epoch 425/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 3636760.7500 - val_loss: 1378431.5000\n",
      "Epoch 426/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1965573.0000 - val_loss: 1240552.7500\n",
      "Epoch 427/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1698022.7500 - val_loss: 1021000.6250\n",
      "Epoch 428/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 4642847.0000 - val_loss: 2780212.0000\n",
      "Epoch 429/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 18434816.0000 - val_loss: 17366728.0000\n",
      "Epoch 430/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 18895382.0000 - val_loss: 43920824.0000\n",
      "Epoch 431/500\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 22394478.0000 - val_loss: 24673848.0000\n",
      "Epoch 432/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 91ms/step - loss: 12396034.0000 - val_loss: 9166140.0000\n",
      "Epoch 433/500\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 12099256.0000 - val_loss: 16386354.0000\n",
      "Epoch 434/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 7820888.0000 - val_loss: 12525755.0000\n",
      "Epoch 435/500\n",
      "10/10 [==============================] - 1s 98ms/step - loss: 31937624.0000 - val_loss: 72795192.0000\n",
      "Epoch 436/500\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 32318404.0000 - val_loss: 52802660.0000\n",
      "Epoch 437/500\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 37415016.0000 - val_loss: 10341779.0000\n",
      "Epoch 438/500\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 7434175.0000 - val_loss: 6341314.5000\n",
      "Epoch 439/500\n",
      "10/10 [==============================] - 1s 84ms/step - loss: 6291622.0000 - val_loss: 5555469.5000\n",
      "Epoch 440/500\n",
      "10/10 [==============================] - 1s 81ms/step - loss: 6201984.0000 - val_loss: 7041868.5000\n",
      "Epoch 441/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 10753619.0000 - val_loss: 13978312.0000\n",
      "Epoch 442/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 8405954.0000 - val_loss: 9071643.0000\n",
      "Epoch 443/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 5410986.5000 - val_loss: 6305517.5000\n",
      "Epoch 444/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 6550647.5000 - val_loss: 5049652.0000\n",
      "Epoch 445/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 5401912.0000 - val_loss: 3184106.2500\n",
      "Epoch 446/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 3679706.5000 - val_loss: 4874263.5000\n",
      "Epoch 447/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 4059166.5000 - val_loss: 5614324.5000\n",
      "Epoch 448/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 4324327.0000 - val_loss: 2576840.7500\n",
      "Epoch 449/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 5694826.5000 - val_loss: 22625680.0000\n",
      "Epoch 450/500\n",
      "10/10 [==============================] - 1s 102ms/step - loss: 7223154.0000 - val_loss: 35329608.0000\n",
      "Epoch 451/500\n",
      "10/10 [==============================] - 1s 92ms/step - loss: 19143678.0000 - val_loss: 10812048.0000\n",
      "Epoch 452/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 5900337.0000 - val_loss: 4766818.0000\n",
      "Epoch 453/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 4745474.5000 - val_loss: 3277134.5000\n",
      "Epoch 454/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 4143308.7500 - val_loss: 4447805.5000\n",
      "Epoch 455/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 3578716.2500 - val_loss: 4132398.0000\n",
      "Epoch 456/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 3160898.5000 - val_loss: 2162932.7500\n",
      "Epoch 457/500\n",
      "10/10 [==============================] - 1s 82ms/step - loss: 2452220.0000 - val_loss: 1497427.0000\n",
      "Epoch 458/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 2559022.7500 - val_loss: 1728772.8750\n",
      "Epoch 459/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 2124810.5000 - val_loss: 4946217.0000\n",
      "Epoch 460/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 2158531.0000 - val_loss: 1922679.8750\n",
      "Epoch 461/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1964197.1250 - val_loss: 1526798.6250\n",
      "Epoch 462/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1845590.2500 - val_loss: 1237486.7500\n",
      "Epoch 463/500\n",
      "10/10 [==============================] - 1s 100ms/step - loss: 1453912.2500 - val_loss: 2181124.7500\n",
      "Epoch 464/500\n",
      "10/10 [==============================] - 1s 99ms/step - loss: 1460406.0000 - val_loss: 897023.2500\n",
      "Epoch 465/500\n",
      "10/10 [==============================] - 1s 95ms/step - loss: 1367575.5000 - val_loss: 872215.8750\n",
      "Epoch 466/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1321691.3750 - val_loss: 1578810.8750\n",
      "Epoch 467/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1299441.3750 - val_loss: 1046330.1875\n",
      "Epoch 468/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1256753.6250 - val_loss: 1410691.3750\n",
      "Epoch 469/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1230556.7500 - val_loss: 1392952.8750\n",
      "Epoch 470/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 1251095.1250 - val_loss: 953808.6250\n",
      "Epoch 471/500\n",
      "10/10 [==============================] - 1s 88ms/step - loss: 1163858.5000 - val_loss: 934838.3125\n",
      "Epoch 472/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1156193.5000 - val_loss: 767604.6875\n",
      "Epoch 473/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1135709.7500 - val_loss: 1036769.8750\n",
      "Epoch 474/500\n",
      "10/10 [==============================] - 1s 87ms/step - loss: 1194570.1250 - val_loss: 887757.0000\n",
      "Epoch 475/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 1256638.7500 - val_loss: 1648417.7500\n",
      "Epoch 476/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1126318.7500 - val_loss: 1548082.1250\n",
      "Epoch 477/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 1067611.2500 - val_loss: 1239565.6250\n",
      "Epoch 478/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 1044525.9375 - val_loss: 1327310.7500\n",
      "Epoch 479/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 1077382.2500 - val_loss: 1598062.5000\n",
      "Epoch 480/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1009614.0000 - val_loss: 1536934.6250\n",
      "Epoch 481/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1032702.6875 - val_loss: 1161379.7500\n",
      "Epoch 482/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 997278.7500 - val_loss: 1092513.5000\n",
      "Epoch 483/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 965913.4375 - val_loss: 1299633.0000\n",
      "Epoch 484/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 970064.4375 - val_loss: 1101103.1250\n",
      "Epoch 485/500\n",
      "10/10 [==============================] - 1s 101ms/step - loss: 948574.3125 - val_loss: 1975043.6250\n",
      "Epoch 486/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 1001038.5000 - val_loss: 1169977.5000\n",
      "Epoch 487/500\n",
      "10/10 [==============================] - 1s 93ms/step - loss: 935971.1875 - val_loss: 1049980.1250\n",
      "Epoch 488/500\n",
      "10/10 [==============================] - 1s 104ms/step - loss: 960236.7500 - val_loss: 1667951.1250\n",
      "Epoch 489/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 988203.0625 - val_loss: 2195562.0000\n",
      "Epoch 490/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 1029707.2500 - val_loss: 821786.5625\n",
      "Epoch 491/500\n",
      "10/10 [==============================] - 1s 85ms/step - loss: 1061584.1250 - val_loss: 1839287.8750\n",
      "Epoch 492/500\n",
      "10/10 [==============================] - 1s 83ms/step - loss: 924702.3750 - val_loss: 870006.1875\n",
      "Epoch 493/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 924691.8125 - val_loss: 1411742.2500\n",
      "Epoch 494/500\n",
      "10/10 [==============================] - 1s 91ms/step - loss: 891758.6250 - val_loss: 1280977.0000\n",
      "Epoch 495/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 900453.1250 - val_loss: 1216381.8750\n",
      "Epoch 496/500\n",
      "10/10 [==============================] - 1s 89ms/step - loss: 956581.1875 - val_loss: 1307609.2500\n",
      "Epoch 497/500\n",
      "10/10 [==============================] - 1s 90ms/step - loss: 935559.9375 - val_loss: 1806965.3750\n",
      "Epoch 498/500\n",
      "10/10 [==============================] - 1s 97ms/step - loss: 932489.0625 - val_loss: 940710.3125\n",
      "Epoch 499/500\n",
      "10/10 [==============================] - 1s 96ms/step - loss: 987714.3125 - val_loss: 1795443.6250\n",
      "Epoch 500/500\n",
      "10/10 [==============================] - 1s 94ms/step - loss: 942133.3125 - val_loss: 1109003.8750\n",
      "3/3 [==============================] - 0s 37ms/step - loss: 683290.1875\n",
      "683290.1875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x18bff95e160>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,  shuffle=False)\n",
    "\n",
    "history=model.fit(X_train, y_train, epochs=500, verbose=1,validation_split=0.2,steps_per_epoch=10)\n",
    "accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(accuracy)\n",
    "figure, axis = plt.subplots()\n",
    "\n",
    "axis.plot(history.history['loss'],'y-')\n",
    "axis.plot(history.history['val_loss'],'g-')\n",
    "\n",
    "#plt.plot(epochs,loss, 'r', label='Training loss')\n",
    "#plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "#plt.title('Training and validation loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5e059378",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c314cf4b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_23800/4282251041.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mtest\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_test_final\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_test_final\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX_test_final\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;36m4\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[0my1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# predicting seven days into the future for each coin\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "test=np.reshape(np.transpose(X_test_final),(1,np.shape(X_test_final)[1],np.shape(X_test_final)[0]))\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    y1=model.predict(test) # predicting seven days into the future for each coin\n",
    "    if i ==0:\n",
    "        predictions=y1\n",
    "    else:\n",
    "        predictions=np.append(predictions,y1, axis=1)\n",
    "    added=np.append(test,y1, axis=1)\n",
    "    adjusted=added[0][7:][:]\n",
    "    adjusted=np.reshape(adjusted, (1,np.shape(adjusted)[0],np.shape(adjusted)[1]))\n",
    "    test=adjusted    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef4bfa1b",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_23800/732522447.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mpredictions\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mreshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[0my_shaped\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my_actual_final\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mpredictions\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mIndexError\u001B[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "predictions=np.reshape(predictions,(np.shape(predictions)[0],np.shape(predictions)[1]))\n",
    "y_shaped=np.transpose(y_actual_final)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e58a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(predictions),np.shape(y_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38ed6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def test_accuracy(predictions,y_shaped):\n",
    "mae=np.mean(np.abs(predictions-y_shaped)) # mean absolute error\n",
    "mse=np.mean(np.square(predictions-y_shaped)) # mean absolute error\n",
    "test_accuracy(predictions,y_shaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a7c2191",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_23800/2682346706.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;31m# 49-84    84-91\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      5\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 6\u001B[1;33m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      7\u001B[0m \u001B[0madded\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest\u001B[0m\u001B[1;33m,\u001B[0m\u001B[0my1\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maxis\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      8\u001B[0m \u001B[0madjusted\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0madded\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m7\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "# 35-70 -   70-77\n",
    "# 35-77\n",
    "# 42-77 -  77-84\n",
    "# 49-84    84-91\n",
    "\n",
    "np.shape(test),np.shape(y1)\n",
    "added=np.append(test,y1, axis=1)\n",
    "adjusted=added[0][7:][:]\n",
    "adjusted=np.reshape(adjusted, (1,np.shape(adjusted)[0],np.shape(adjusted)[1]))\n",
    "test=adjusted\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9d100d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_23800/3455244757.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0my1\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpredict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtest\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;31m# predicting seven days into the future for each coin\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      2\u001B[0m \u001B[1;31m#predictions.append(y1)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[0mi\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0my1\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;31m# day one for all coins\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[0mi\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m20\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "y1=model.predict(test) # predicting seven days into the future for each coin\n",
    "#predictions.append(y1)\n",
    "i = y1[0][0] # day one for all coins\n",
    "i[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fc5394",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48448b09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(X_test_final), np.shape(X_train)\n",
    "\n",
    "c=np.reshape(np.transpose(X_test_final),(1,np.shape(X_test_final)[1],np.shape(X_test_final)[0]))\n",
    "np.shape(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f20027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_actual_final[0][0:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5392ff3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_23800/2222823026.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mpredictions\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22bd5ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}